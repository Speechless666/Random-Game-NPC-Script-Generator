# -*- coding: utf-8 -*-
# project/eval/auto_eval.py
# LLM-as-a-Judge è‡ªåŠ¨åŒ–è¯„ä¼°å·¥å…·ï¼Œç”¨äºé‡åŒ–æŠ¤æ æ•ˆæœä¸ç¨³å®šæ€§ã€‚
from __future__ import annotations
import argparse, glob, json, os, csv, hashlib, sqlite3, time, sys, pathlib
from typing import Any, Dict, List, Tuple
import statistics
import matplotlib.pyplot as plt  # å‡è®¾å·²å®‰è£…

# å¯¼å…¥ QwenProvider å’Œ APIError (å…¼å®¹è·¯å¾„)
try:
    from project.provider.qwen import QwenProvider, APIError
except Exception:
    THIS = pathlib.Path(__file__).resolve()
    PROJECT_DIR = THIS.parents[1]
    sys.path.insert(0, str(PROJECT_DIR))
    try:
        from provider.qwen import QwenProvider, APIError  # type: ignore
    except Exception:
        class QwenProvider:  # type: ignore
            def __init__(self, *args, **kwargs):
                raise APIError("QwenProvider Import Failed or QWEN_API_KEY missing.")


        class APIError(Exception):
            pass  # type: ignore


# å¯¼å…¥ç¼“å­˜ï¼ˆå¦‚æœæ‚¨å·²å®ç° project/runtime/cache.pyï¼Œè¯·æ›¿æ¢ä¸‹é¢çš„ KVCacheï¼‰
class KVCache:
    # ç®€æ˜“ SQLite ç¼“å­˜å®ç°
    def __init__(self, path="project/eval/.llm_cache.sqlite3"):
        os.makedirs(os.path.dirname(path), exist_ok=True)
        self.path = path
        import sqlite3 as _sq
        self.conn = _sq.connect(self.path)
        with self.conn:
            self.conn.execute("CREATE TABLE IF NOT EXISTS kv (k TEXT PRIMARY KEY, v TEXT NOT NULL, ts REAL);")

    def get(self, k: str):
        with self.conn:
            cur = self.conn.execute("SELECT v FROM kv WHERE k=?;", (k,))
            row = cur.fetchone()
            if not row: return None
            try:
                return json.loads(row[0])
            except Exception:
                return None

    def set(self, k: str, v: Any):
        with self.conn:
            self.conn.execute("INSERT OR REPLACE INTO kv(k, v, ts) VALUES (?, ?, ?);",
                              (_json(k), _json(v), time.time()))


# ---- è¾…åŠ©å‡½æ•° (ä¿æŒä¸å˜) ----
def _read_jsonl(globs: List[str]) -> List[Dict[str, Any]]:
    files: List[str] = []
    for pat in globs:
        files.extend(sorted(glob.glob(pat)))
    rows = []
    for f in files:
        try:
            with open(f, "r", encoding="utf-8") as fh:
                for line in fh:
                    line = line.strip()
                    if not line: continue
                    try:
                        rows.append(json.loads(line))
                    except Exception:
                        pass
        except FileNotFoundError:
            continue
    return rows


def _json(obj: Any) -> str:
    return json.dumps(obj, ensure_ascii=False, separators=(",", ":"), default=str)


def _sha(s: str) -> str:
    import hashlib as _hl
    return _hl.sha256(s.encode("utf-8")).hexdigest()[:24]


# ----------------- æ—¥å¿—å­—æ®µè§£æ (ä¿æŒä¸å˜) -----------------
def _as_dict(x: Any) -> Dict[str, Any]:
    return x if isinstance(x, dict) else {}


def _as_str(x: Any) -> str:
    return x if isinstance(x, str) else ""


def _get_final_text(rec: Dict[str, Any]) -> str:
    f = rec.get("final")
    if isinstance(f, dict): return _as_str(f.get("text"))
    if isinstance(f, str): return f
    return _as_str(rec.get("text"))


def _get_proposed_emotion(rec: Dict[str, Any]) -> str:
    ep = rec.get("emotion_proposed")
    if isinstance(ep, dict): return _as_str(ep.get("emotion"))
    if isinstance(ep, str) and ep.strip(): return ep.strip()
    ph = rec.get("pre_hint")
    if isinstance(ph, dict): return _as_str(ph.get("emotion"))
    if isinstance(ph, str) and ph.strip(): return ph.strip()
    f = rec.get("final")
    if isinstance(f, dict):
        emo = _as_str(f.get("emotion"))
        if emo: return emo
    return ""


def _get_evidence_or_ids(rec: Dict[str, Any]) -> Any:
    if "evidence" in rec: return rec["evidence"]
    if "evidence_ids" in rec: return rec["evidence_ids"]
    return None


def _get_latency(rec: Dict[str, Any]) -> float:
    lat_ms = rec.get("latency_ms")
    if isinstance(lat_ms, (int, float)):
        return float(lat_ms) / 1000.0
    return 0.0


# ----------------- å›¾è¡¨ç”Ÿæˆå·¥å…· (ä¿æŒä¸å˜) -----------------
def _generate_charts(summary: Dict[str, Any], ooc_scores: List[float], leak_flags: List[int], emo_real_flags: List[int],
                     out_dir: str):
    """æ ¹æ®è¯„ä¼°ç»“æœç”Ÿæˆå¹¶ä¿å­˜å›¾è¡¨ã€‚"""
    if plt is None:
        print("âš ï¸ Matplotlib is not available. Skipping chart generation.", file=sys.stderr)
        return

    os.makedirs(out_dir, exist_ok=True)

    # 1. OOC Risk åˆ†å¸ƒå›¾ (ç›´æ–¹å›¾)
    if ooc_scores:
        plt.figure(figsize=(7, 5))
        plt.hist(ooc_scores, bins=20, range=(0, 1.0), edgecolor='black', alpha=0.7)
        plt.title('Out-of-Character Risk Distribution')
        plt.xlabel('OOC Risk Score (0.0 - 1.0)')
        plt.ylabel('Frequency')
        plt.grid(axis='y', alpha=0.5)
        ooc_path = os.path.join(out_dir, 'ooc_risk_distribution.png')
        plt.savefig(ooc_path)
        plt.close()
        summary['chart_ooc'] = ooc_path
        print(f"âœ… OOC Risk Chart saved to: {ooc_path}", file=sys.stderr)

    # 2. æ€»ç»“æŒ‡æ ‡æ¡å½¢å›¾ (Leak Rate, Emotion Realization Rate)
    labels = []
    values = []

    if leak_flags:
        leak_rate = summary.get('llm_leak_rate', sum(leak_flags) / len(leak_flags) if leak_flags else 0)
        labels.append('Leak Rate')
        values.append(leak_rate)

    if emo_real_flags:
        emo_rate = summary.get('llm_emotion_realization',
                               sum(emo_real_flags) / len(emo_real_flags) if emo_real_flags else 0)
        labels.append('Emotion Realization')
        values.append(emo_rate)

    if labels:
        plt.figure(figsize=(6, 5))
        bars = plt.bar(labels, values, color=['#ff9999', '#66b3ff'])
        plt.ylim(0, 1.05)
        plt.title('Summary Evaluation Metrics')
        plt.ylabel('Rate (0.0 - 1.0)')
        plt.grid(axis='y', alpha=0.5)

        for bar in bars:
            yval = bar.get_height()
            plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.05,
                     f'{yval:.2%}', ha='center', va='bottom')

        summary_path = os.path.join(out_dir, 'summary_metrics_bar.png')
        plt.savefig(summary_path)
        plt.close()
        summary['chart_summary'] = summary_path
        print(f"âœ… Summary Metrics Chart saved to: {summary_path}", file=sys.stderr)


# ----------------- ä¸»ç¨‹åº -----------------
def main():
    ap = argparse.ArgumentParser(description="LLM-as-a-Judge è‡ªåŠ¨åŒ–è¯„ä¼°å·¥å…·ï¼Œæ”¯æŒå¤šæŒ‡æ ‡é‡åŒ–å’Œ CI/CD æŠ¤æ ã€‚")

    # æ ¸å¿ƒè¾“å…¥/è¾“å‡º
    ap.add_argument("--logs", nargs="+", required=True, help="Glob(s) to JSONL logs, e.g. project/logs/npc-*.jsonl")
    ap.add_argument("--forbidden", type=str, default=None, help="forbidden.txt (optional, used for leak judge)")
    ap.add_argument("--out_json", type=str, default="project/eval/auto_eval_summary.json",
                    help="è¾“å‡ºæ±‡æ€» JSON æ–‡ä»¶è·¯å¾„.")
    ap.add_argument("--out_csv", type=str, default="project/eval/auto_eval_detailed.csv", help="è¾“å‡ºè¯¦ç»† CSV æ–‡ä»¶è·¯å¾„.")

    # æ–°å¢æ•°æ®å’Œé…ç½®å‚æ•° (åŒ¹é…ç”¨æˆ·è¾“å…¥)
    ap.add_argument("--bad_examples", type=str, default=None, help="List of bad example responses (optional).")
    ap.add_argument("--special_phrases", type=str, default=None, help="List of phrases that must or must not be used.")

    # LLM è¯„å§”é€‰æ‹©
    ap.add_argument("--judge", type=str, default="all",
                    choices=["ooc", "leak", "emotion", "consistency", "repetition", "latency", "all"],
                    help="é€‰æ‹©è¦è¿è¡Œçš„è¯„ä¼°æŒ‡æ ‡.")

    # é˜ˆå€¼å’Œ CI/CD æŠ¤æ å‚æ•° (åŒ¹é…ç”¨æˆ·è¾“å…¥)
    ap.add_argument("--ooc_threshold", type=float, default=0.5, help="OOC risk score threshold to trigger CI failure.")
    ap.add_argument("--emotion_consistency_min", type=float, default=0.85,
                    help="Minimum emotion consistency rate required for passing.")
    ap.add_argument("--fail_on_regress", action="store_true", help="å¦‚æœå¯ç”¨ï¼Œå°†å¯¹æ¯”åŸºçº¿å¹¶æ£€æµ‹æŒ‡æ ‡æ˜¯å¦æ¶åŒ–.")
    ap.add_argument("--fail_on_threshold", action="store_true",
                    help="å½“ OOC æˆ– Leak ç­‰æ ¸å¿ƒæŒ‡æ ‡è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œä»¥éé›¶ä»£ç é€€å‡º.")

    args = ap.parse_args()

    rows = _read_jsonl(args.logs)
    if not rows:
        print(_json({"n_rows": 0}))
        sys.exit(0)

    # åŠ è½½æ•æ„Ÿè¯åˆ—è¡¨
    forbidden_list: List[str] = []
    if args.forbidden and os.path.exists(args.forbidden):
        with open(args.forbidden, "r", encoding="utf-8") as f:
            forbidden_list = [x.strip() for x in f if x.strip()]

    # â— åˆå§‹åŒ– Provider
    try:
        provider = QwenProvider()
    except APIError as e:
        summary = {"n_rows": len(rows), "errors": len(rows), "fatal_error": str(e)}
        os.makedirs(os.path.dirname(args.out_json), exist_ok=True)
        with open(args.out_json, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        print(f"è‡´å‘½é”™è¯¯ï¼šProvider åˆå§‹åŒ–å¤±è´¥ã€‚{_json(summary)}", file=sys.stderr)
        sys.exit(1)

    cache = KVCache()

    details: List[Dict[str, Any]] = []
    ooc_scores: List[float] = []
    leak_flags: List[int] = []
    emo_real_flags: List[int] = []
    latencies: List[float] = []
    repetition_flags: List[int] = []

    error_count = 0

    for i, r in enumerate(rows):
        # ... (æ•°æ®æå–å’Œ LLM è°ƒç”¨é€»è¾‘ä¿æŒä¸å˜ï¼Œç”¨äº OOC, Leak, Emotion) ...
        rec: Dict[str, Any] = {
            "timestamp": r.get("timestamp"),
            "session_id": r.get("session_id"),
            "turn_id": r.get("turn_id"),
            "slot": r.get("slot"),
            "final_text": _get_final_text(r),
            "proposed_emotion": _get_proposed_emotion(r),
        }

        # æå–å»¶è¿Ÿ (æ— éœ€ LLM)
        if args.judge in ("latency", "all"):
            latency_sec = _get_latency(r)
            rec["latency_sec"] = latency_sec
            if latency_sec > 0:
                latencies.append(latency_sec)

        # æå–é‡å¤ç‡ (ç®€åŒ–ç‰ˆæœ¬)
        if args.judge in ("repetition", "all") and i > 0 and details:
            prev_final_text = details[-1].get("final_text", "")
            is_repeated = 1 if rec["final_text"] == prev_final_text and rec["final_text"] else 0
            rec["is_repeated"] = bool(is_repeated)
            repetition_flags.append(is_repeated)

        try:
            final_text = rec["final_text"]
            proposed = rec["proposed_emotion"]
            context = {
                "slot": r.get("slot"),
                "evidence": _get_evidence_or_ids(r),
                "ctx": r.get("ctx"),
            }

            # ---- LLM-as-a-Judge: OOC, Leak, Emotion ----
            # OOC
            if args.judge in ("ooc", "all"):
                key = "ooc:" + _sha(_json(context) + "||" + final_text)
                ooc = cache.get(key)
                if ooc is None:
                    ooc = provider.judge_ooc(context=_json(context), output=final_text)
                    cache.set(key, ooc)
                rec.update({"llm_ooc_risk": float(ooc.get("ooc_risk", 0.0)), "llm_ooc_reasons": ooc.get("reasons", [])})
                ooc_scores.append(float(ooc.get("ooc_risk", 0.0)))

            # Leak
            if args.judge in ("leak", "all"):
                key = "leak:" + _sha(_json(forbidden_list) + "||" + final_text)
                leak = cache.get(key)
                if leak is None:
                    leak = provider.judge_leak(forbidden_list=forbidden_list, output=final_text)
                    cache.set(key, leak)
                leak_flag = 1 if leak.get("leak") else 0
                rec.update({"llm_leak": bool(leak.get("leak", False)), "llm_leak_hits": leak.get("hits", [])})
                leak_flags.append(leak_flag)

            # Emotion Realization
            if args.judge in ("emotion", "all"):
                key = "emo:" + _sha((proposed or "") + "||" + final_text)
                emo = cache.get(key)
                if emo is None:
                    emo = provider.judge_emotion(proposed=proposed or "", output=final_text)
                    cache.set(key, emo)
                realized = bool(emo.get("realized", False))
                rec.update({"llm_emotion_realized": realized, "llm_emotion_evidence": emo.get("evidence", [])})
                emo_real_flags.append(1 if realized else 0)

            # TODO: Future Judge: Consistency/Memory (ä½¿ç”¨ provider.judge_consistency ç­‰)
            if args.judge in ("consistency", "all"):
                pass

            details.append(rec)

        except APIError as e:
            error_count += 1
            rec.update({"error": f"Qwen API Error: {e}", "raw_sample_truncated": _json(r)[:1000]})
            details.append(rec)
            continue
        except Exception as e:
            error_count += 1
            rec.update({"error": f"{type(e).__name__}: {e}", "raw_sample_truncated": _json(r)[:1000]})
            details.append(rec)
            continue

    # ---- æ±‡æ€»è®¡ç®— ----
    summary: Dict[str, Any] = {"n_rows": len(rows), "errors": error_count}

    # è®¡ç®—å„é¡¹æŒ‡æ ‡å‡å€¼
    if ooc_scores: summary["llm_ooc_mean"] = round(sum(ooc_scores) / len(ooc_scores), 4)
    if leak_flags: summary["llm_leak_rate"] = round(sum(leak_flags) / len(leak_flags), 4)
    if emo_real_flags: summary["llm_emotion_realization"] = round(sum(emo_real_flags) / len(emo_real_flags), 4)
    if latencies:
        summary["mean_latency_sec"] = round(statistics.mean(latencies), 4)
        summary["median_latency_sec"] = round(statistics.median(latencies), 4)
    if repetition_flags:
        summary["repetition_rate"] = round(sum(repetition_flags) / (len(rows) - 1), 4) if len(rows) > 1 else 0.0

    # â— ç”Ÿæˆå›¾è¡¨
    chart_output_dir = os.path.join(os.path.dirname(args.out_json), 'charts')
    if details:
        _generate_charts(
            summary=summary,
            ooc_scores=ooc_scores,
            leak_flags=leak_flags,
            emo_real_flags=emo_real_flags,
            out_dir=chart_output_dir
        )

    # å†™å…¥ JSON å’Œ CSV æ–‡ä»¶
    os.makedirs(os.path.dirname(args.out_json), exist_ok=True)
    with open(args.out_json, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    os.makedirs(os.path.dirname(args.out_csv), exist_ok=True)
    if details:
        all_keys = set()
        for d in details:
            all_keys.update(d.keys())
        try:
            with open(args.out_csv, "w", encoding="utf-8", newline="") as f_csv:
                writer = csv.DictWriter(f_csv, fieldnames=sorted(list(all_keys)))
                writer.writeheader()
                writer.writerows(details)
        except Exception as e:
            print(f"å†™å…¥ CSV æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}", file=sys.stderr)

    print(_json(summary))

    # ----------------- CI/CD é˜ˆå€¼å‘Šè­¦ (æŠ¤æ é€»è¾‘) -----------------
    if args.fail_on_threshold:
        should_fail = False

        # 1. OOC é˜ˆå€¼æ£€æŸ¥
        ooc_mean = summary.get("llm_ooc_mean", 0)
        if ooc_mean > args.ooc_threshold:
            print(f"ğŸš¨ å‘Šè­¦: OOC é£é™© ({ooc_mean:.4f}) è¶…è¿‡é˜ˆå€¼ {args.ooc_threshold:.2f}.", file=sys.stderr)
            should_fail = True

        # 2. Leak é˜ˆå€¼æ£€æŸ¥ (ç¡¬ç¼–ç ä¸€ä¸ªé»˜è®¤çš„ Leak é˜ˆå€¼ï¼Œå› ä¸ºç”¨æˆ·æ²¡æœ‰åœ¨å‘½ä»¤è¡Œæä¾›)
        LEAK_FAIL_THRESHOLD = 0.05
        leak_rate = summary.get("llm_leak_rate", 0)
        if leak_rate > LEAK_FAIL_THRESHOLD:
            print(f"ğŸš¨ å‘Šè­¦: æ³„å¯†ç‡ ({leak_rate:.4f}) è¶…è¿‡é˜ˆå€¼ {LEAK_FAIL_THRESHOLD:.2f}.", file=sys.stderr)
            should_fail = True

        # 3. Emotion Consistency æœ€å°è¦æ±‚æ£€æŸ¥
        # è­¦å‘Šï¼šæ­¤å¤„çš„ llm_emotion_realization ä¸ emotion_consistency_min åœ¨è¯­ä¹‰ä¸Šç•¥æœ‰ä¸åŒï¼Œ
        # ä½†æˆ‘ä»¬ä½¿ç”¨å·²å®ç°çš„ Realization æ›¿ä»£ Consistency è¿›è¡Œæ£€æŸ¥ã€‚
        emo_real = summary.get("llm_emotion_realization", 0)
        if emo_real < args.emotion_consistency_min:
            print(f"ğŸš¨ å‘Šè­¦: æƒ…æ„Ÿå®ç°ç‡ ({emo_real:.4f}) ä½äºæœ€ä½è¦æ±‚ {args.emotion_consistency_min:.2f}.",
                  file=sys.stderr)
            should_fail = True

        # 4. é”™è¯¯è®¡æ•°æ£€æŸ¥
        if error_count > 0:
            print(f"ğŸš¨ å‘Šè­¦: LLM API è°ƒç”¨å­˜åœ¨ {error_count} ä¸ªé”™è¯¯.", file=sys.stderr)
            should_fail = True

        if should_fail:
            print("âŒ è‡ªåŠ¨è¯„ä¼°å› é˜ˆå€¼å¤±è´¥ï¼Œé€€å‡ºç  1ã€‚", file=sys.stderr)
            sys.exit(1)

    # TODO: é¢„ç•™ç»™ --fail_on_regress é€»è¾‘
    if args.fail_on_regress:
        # åœ¨æ­¤å¤„æ·»åŠ è¯»å–åŸºçº¿æŠ¥å‘Šå¹¶å¯¹æ¯”æœ¬æ¬¡ summary çš„é€»è¾‘
        print("âš ï¸ fail_on_regress åŠŸèƒ½å°šæœªå®ç°å¯¹æ¯”é€»è¾‘ã€‚", file=sys.stderr)


if __name__ == "__main__":
    main()